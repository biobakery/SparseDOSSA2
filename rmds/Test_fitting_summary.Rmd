---
title: "Does lambda matter?"
author: "Siyuan Ma"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
dir_project <- "/n/hutlab11_nobackup/users/syma/sparsedossa_update/"
# dir_project <- "~/Dropbox (Harvard University)/sparsedossa_update/"
knitr::opts_knit$set(root.dir = dir_project)
library(magrittr)
library(ggplot2)
dir_output <- paste0(dir_project, "results/Test_fitting/")
dir.create(dir_output, recursive = TRUE, showWarnings = FALSE)
```

```{r read in files, include=FALSE}
rescale_log <- function(x) {
  log10(abs(x)) * sign(x)
}

load(paste0(dir_output, "tb_simulation.RData"))
tb_simulation <- tb_simulation %>% 
  dplyr::arrange(i_setup) %>% 
  dplyr::mutate(N_p = paste0("n=", n, 
                             ",p=", p) %>% 
                  forcats:::as_factor(),
                pi0_Sigma_off_diag = paste0("pi0=", pi0,
                                            ",rho=", Sigma_off_diag) %>% 
                  forcats:::as_factor())

result_files <- list.files(dir_output, pattern = "^result")
i_results <- result_files %>% 
  gsub("result_", "", ., fixed = TRUE) %>% 
  gsub(".RData", "", ., fixed = TRUE) %>% 
  as.numeric()
fit_files <- list.files(dir_output, pattern = "^fit")
i_fit <- fit_files %>% 
  gsub("fit_EM_", "", ., fixed = TRUE) %>% 
  gsub(".RData", "", ., fixed = TRUE) %>% 
  as.numeric()
setdiff(i_fit, i_results)[1]
# l_results <- list()
# for(i_file in seq_along(result_files)) {
#   i <- result_files[i_file] %>%
#     gsub("result_", "", ., fixed = TRUE) %>%
#     gsub(".RData", "", ., fixed = TRUE) %>%
#     as.numeric
#   load(paste0(dir_output, result_files[i_file]))
#   load(paste0(dir_output, "fit_EM_", i, ".RData"))
#   result$i <- i
#   result$converge <- fit_EM$converge
#   result$diff <- fit_EM$ll_params[[length(fit_EM$ll_params)]]$diff
#   result$params_x$truth$Corr_off_diag <- result$params_x$truth$Corr[lower.tri(result$params_x$truth$Corr)]
#   result$params_x$fit$Corr_off_diag <- result$params_x$fit$Corr[lower.tri(result$params_x$fit$Corr)]
#   result$l <- fit_EM$ll_params[[length(fit_EM$ll_params)]]$l
# 
#   l_results <- c(l_results, list(result))
# }
# save(l_results, file = paste0(dir_output, "l_results.RData"))
load(paste0(dir_output, "l_results.RData"))
```

# Does varying $\lambda$ actually change model esitmation?
First let's recap our model parameterization:

\begin{align}
f_A(a_1, \dots, a_p) &= \prod f_{A_j}(a_j | \pi_j, \mu_j, \sigma_j) \times f_U(F_{A_1}(a_1), \dots, F_{A_p}(a_p) | \Omega)
\end{align}

* $f_{A_j}(a_j | \pi_j, \mu_j, \sigma_j)$ are zero-inflated log normal marginals 
* $f_U(u_1, \dots, u_p | \Omega)$ is Gaussian copula for specifying correlation structure between $A_j$.

When all $\pi_j$ are zeros, i.e., there is no zero inflation, this model simplifies into logistic multivariate normal model. For the logistic MVN, the induced pdf for $X (= \frac{A}{\sum A})$ is analytical [Fang et al., 2017](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5510714/):

\begin{align}
f_X &= (2 \pi)^{-\frac{p - 1}{2}} \left(\frac{|\Omega^*|}{1_p^\prime \Omega^* 1_p}\right)^{\frac{1}{2}}\exp\left\{-\frac{1}{2}(F_0 \ln x - F_0 \mu)^\prime \left(\Omega^* - \frac{\Omega^* 1_p 1_p^\prime \Omega^*}{1^\prime_p \Omega^* 1_p}\right)(F_0 \ln x - F_0 \mu)\right\}
\end{align}

Where $\Omega^* = \text{diag}(1/\sigma_1, \dots, 1/\sigma_p) \cdot \Omega^* \cdot \text{diag}(1/\sigma_1, \dots, 1/\sigma_p)$ is the covariance matrix for $\log A$ in this setting. $F_0 = I - \frac{1}{n}1_p 1_p^\prime$. We use this special case to study the choice of $\lambda$, which enforces different sparsity in $\hat{\Omega}$, on our model estimations.

We plot the data log-likelihood $\frac{1}{n}\log \sum f_X(x_i | \hat{\mu}, \hat{\sigma}_\lambda, \hat{\Omega}_\lambda)$ across different $\lambda$s (note that $\hat{\mu}$ does not change with $\lambda$). It seems that the likelihood does not really change with $\lambda$. This lead me to think that maybe even though $\lambda$ leads to different $\hat{\sigma}_\lambda, \hat{\Omega}_\lambda$, together they might still induce the same $f_X$. 

```{r log likelihood, fig.width=15, fig.height=12, echo = FALSE}
tb_results_ll <-
    tibble::tibble(
      i_setup = l_results %>%
        purrr::map_dbl("i"),
      Converge = l_results %>%
            purrr::map_lgl(~.x$converge$converge),
      ll = l_results %>%
            purrr::map_dbl("l")
    )  %>%
  dplyr::filter(!is.na(Converge)) %>%
  dplyr::filter(i_setup <= max(i_setup, na.rm = TRUE)) %>%
  dplyr::left_join(tb_simulation, by = "i_setup")

p_l <- tb_results_ll %>%
  dplyr::filter(pi0 == 0) %>%
  ggplot(aes(x = log10(lambda), y = ll, color = as.factor(R))) +
  geom_point() +
  geom_line() +
  # scale_alpha_manual(values = c("TRUE"=1, "FALSE"=0.7),
  #                    na.value = "grey") +
  facet_wrap( ~ N_p + pi0_Sigma_off_diag, scales = "free_y", ncol = 3) +
  ggtitle("Log likelihood")
print(p_l)
```

The terms in $f_X$ that involves $\sigma$ and $\Omega$ are the scalar $c = \frac{|\Omega^*|}{1_p^\prime \Omega^* 1_p}$ and the matrix $C = \Omega^* - \frac{\Omega^* 1_p 1_p^\prime \Omega^*}{1^\prime_p \Omega^* 1_p}$, so I plotted $\hat{c}_\lambda$, $\hat{C}_{\lambda, 11}$, and $\hat{C}_{\lambda, 12}$ as well.

The most noticeable variation is for $\hat{C}_{\lambda, 12}$ in the scenario of $n = 100, p = 10$. As $\lambda$ increases $\hat{C}_{\lambda, 12}$ from different simulation runs seem to be converging, presumably due to $\hat{\Omega}_\lambda$ being forced to diagonal. Going back the log likelihood plots, this is also the scenario where there are indeed noticeable decrease in the likelihood as well!

```{r absolute abundance params, fig.width=15, fig.height=12, echo=FALSE}
centralize_Omega <- function(Sigma) {
  Omega <- solve(Sigma)
  Omega - Omega %*% matrix(1, nrow = nrow(Omega), ncol = ncol(Omega)) %*% Omega / sum(Omega)
}
normalize_Omega <- function(Sigma) {
  Omega <- solve(Sigma)
  det(Omega) / sum(Omega)
}
#
tb_results_a <-
  tibble::tibble(
    i_setup = l_results %>%
      purrr::map_dbl("i"),
    Est_sigma = l_results %>%
      purrr::map(~.x$params_a$fit[["sigma"]]),
    Est_Sigma = l_results %>%
      purrr::map(~solve(.x$params_a$fit[["Omega"]])),
    Converge = l_results %>%
      purrr::map(~.x$converge$converge)
  ) %>%
  dplyr::filter(!is.na(Converge)) %>%
  dplyr::filter(i_setup <= max(i_setup, na.rm = TRUE)) %>%
  dplyr::left_join(tb_simulation, by = "i_setup") %>%
  dplyr::filter(pi0 == 0) %>%
  dplyr::mutate(Est_Var = Est_sigma %>%
                  purrr::map2(Est_Sigma,
                              ~ diag(.x) %*% .y %*% diag(.x)),
                Est_Var_Identified = Est_Var %>%
                  purrr::map(centralize_Omega),
                Est_Var_Norm = Est_Var %>%
                  purrr::map_dbl(normalize_Omega)) %>%
  dplyr::mutate(Ele_11 = Est_Var_Identified %>%
                  purrr::map_dbl(~.x[1, 1]),
                Ele_12 = Est_Var_Identified %>%
                  purrr::map_dbl(~.x[1, 2]))
# 
p_11 <-  tb_results_a %>%
  dplyr::filter(pi0 == 0) %>%
  ggplot(aes(x = log10(lambda), y = Ele_11, color = as.factor(R))) +
  geom_point() +
  geom_line() +
  facet_wrap( ~ N_p + pi0_Sigma_off_diag, scales = "free_y", ncol = 3) +
  ggtitle("C_11")

p_12 <-  tb_results_a %>%
  dplyr::filter(pi0 == 0) %>%
  ggplot(aes(x = log10(lambda), y = Ele_12, color = as.factor(R))) +
  geom_point() +
  geom_line() +
  facet_wrap( ~ N_p + pi0_Sigma_off_diag, scales = "free_y", ncol = 3) +
  ggtitle("C_12")

p_norm <-  tb_results_a %>%
  dplyr::filter(pi0 == 0) %>%
  ggplot(aes(x = log10(lambda), y = Est_Var_Norm, color = as.factor(R))) +
  geom_point() +
  geom_line() +
  facet_wrap( ~ N_p + pi0_Sigma_off_diag, scales = "free_y", ncol = 3) +
  ggtitle("c")

print(p_norm)
print(p_11)
print(p_12)
```

# Conclusion

So it seems in a lot of situations, $\hat{\sigma}_\lambda$ can adapt to $\hat{\Omega}_\lambda$, inducing the same $f_X(\cdot|\hat{\sigma}_\lambda, \hat{\Omega}_\lambda)$. I think this goes back to the notion of using $L_1$ to solve unidentifiability - if the penalization is only picking one solution out of an equivalence class, then switching $\lambda$ is just picking out different solutions from the same class, hence model performance such as likelihood should not change?

Of course - penalization in this context also has utility when $p \geq n$. In that context maybe indeed varying $\lambda$ can pick a better model?

# Full set of fitting performance

The run on $n = 1000$ scenarios also finished running, with quite some cases that timed out. I'll have to debug more on that. But on the finished results (plotted are the relative difference between the first element of estimated $X$ mean, variance, and Spearman correlation versus the truth: 

*Relative difference does decrease with sample size, which is good news? 
*It is however concerning that the correlation of $X$ was estimated so wrong when $p = 10$, too.

```{r set up simulation grid, fig.width=16, fig.height=12, echo=FALSE}
get_abs_diff <- function(x, y) y - x
get_rel_diff <- function(x, y, denom_c = 1e-5) (y - x) / (abs(x) + denom_c)

tb_results_x <-
  c("mu", "sigma", "Corr_off_diag") %>%
  purrr::map_dfr(function(param) {
    tibble::tibble(
      i_setup = tb_simulation$i_setup,
      Param = param
    ) %>%
      dplyr::left_join(
        tibble::tibble(
          i_setup = l_results %>%
            purrr::map_dbl("i"),
          Truth = l_results %>%
            purrr::map(~.x$params_x$truth[[param]]),
          SparseDOSSA2 = l_results %>%
            purrr::map(~.x$params_x$fit[[param]]),
          Converge = l_results %>%
            purrr::map_lgl(~.x$converge$converge)
        ),
        by = "i_setup"
      )
  }) %>%
  dplyr::filter(!is.na(Converge)) %>%
  dplyr::filter(i_setup <= max(i_setup, na.rm = TRUE)) %>%
  dplyr::group_by(i_setup, Param) %>%
  dplyr::mutate(
    abs_diff = list({
      if(is.null(Truth[[1]]))
        NA
      else
        purrr::map2_dbl(as.vector(Truth[[1]]), as.vector(SparseDOSSA2[[1]]),
                        ~ get_abs_diff(.x, .y), method = "abs")
    }),
    rel_diff = list({
      if(is.null(Truth[[1]]))
        NA
      else
        purrr::map2_dbl(as.vector(Truth[[1]]), as.vector(SparseDOSSA2[[1]]),
                        ~ get_rel_diff(.x, .y), method = "rel")
    })
  ) %>%
  dplyr::ungroup() %>%
  dplyr::left_join(tb_simulation, by = "i_setup")
```

```{r one element, fig.width=15, fig.height=12, echo=FALSE}
# plot one element
for(i_param in c("mu", "sigma", "Corr_off_diag")) {
  p_fit <- tb_results_x %>%
    dplyr::filter(Param == i_param) %>%
    dplyr::mutate(rel_diff_1 = rel_diff %>% purrr::map_dbl(1)) %>%
    ggplot(aes(x = log10(lambda), y = rel_diff_1, color = as.factor(R),
               alpha = Converge)) +
    geom_point() +
    geom_line() +
    # scale_alpha_manual(values = c("TRUE"=1, "FALSE"=0.7),
    #                    na.value = "grey") +
    facet_grid(N_p ~ pi0_Sigma_off_diag, scales = "free_y") +
    ggtitle(paste0(i_param, "_x"))
  print(p_fit)
}
```